{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317c788-df0f-4eda-b799-436983ad42be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242480c9-9426-4444-ab36-2e74f4fc2b18",
   "metadata": {},
   "source": [
    "# Noisy Partials\n",
    "\n",
    "This notebook explores the effect of noisy partials and whether or not the hyperparameter $\\gamma$ can mitigate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8c6dd-1d56-40af-9a37-822f0a286899",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f67b76-a0cb-48aa-8d38-452857312249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import jenn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a37da-416c-4e57-ad32-c7fb6d99ea7c",
   "metadata": {},
   "source": [
    "### Notebook Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c8c22-2394-4c3b-a293-5c1e2c4519ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_training_data(step_size: float = 1e-6, seed: int = 123):\n",
    "    \"\"\"Generate training data using finite difference.\"\"\"\n",
    "    test_function = jenn.synthetic.Rastrigin\n",
    "    lb = (-1.0, -1.0)\n",
    "    ub = (1.5, 1.5)\n",
    "    X_train, Y_train, J_train = test_function.sample(\n",
    "        m_random=100,\n",
    "        m_levels=0,\n",
    "        lb=lb,\n",
    "        ub=ub,\n",
    "        dx=None,\n",
    "        random_state=seed,\n",
    "    )  # analytical partials\n",
    "    X_train_FD, Y_train_FD, J_train_FD = test_function.sample(\n",
    "        m_random=100,\n",
    "        m_levels=0,\n",
    "        lb=lb,\n",
    "        ub=ub,\n",
    "        dx=step_size,\n",
    "        random_state=seed,\n",
    "    )  # finite difference\n",
    "    X_test, Y_test, J_test = test_function.sample(\n",
    "        m_random=0,\n",
    "        m_levels=10,\n",
    "        lb=lb,\n",
    "        ub=ub,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    return (\n",
    "        (X_train, Y_train, J_train),\n",
    "        (X_train_FD, Y_train_FD, J_train_FD),\n",
    "        (X_test, Y_test, J_test),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cb6c4-df6c-4106-b8b2-890ccd48ab62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_rastrigin(step_size: float = 1e-6, gamma: float = 1.0, seed: int = 123):\n",
    "    \"\"\"Generate training data and fit Rastrigin function.\n",
    "\n",
    "    :param step_size: finite difference step size for training data partials\n",
    "    \"\"\"\n",
    "    # Training data\n",
    "    (\n",
    "        (X_train, Y_train, _),\n",
    "        (_, _, J_train_FD),\n",
    "        (X_test, Y_test, _),\n",
    "    ) = generate_training_data(step_size)\n",
    "\n",
    "    # Train neural net\n",
    "    model = jenn.NeuralNet(\n",
    "        layer_sizes=[2, 24, 24, 1],\n",
    "    ).fit(\n",
    "        x=X_train,\n",
    "        y=Y_train,\n",
    "        dydx=J_train_FD,\n",
    "        max_iter=5_000,\n",
    "        alpha=0.001,\n",
    "        lambd=0.001,\n",
    "        gamma=gamma,\n",
    "        is_normalize=True,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    # Goodness of fit\n",
    "    r2 = jenn.metrics.rsquare(\n",
    "        y_pred=model.predict(X_test),\n",
    "        y_true=Y_test,\n",
    "    ).squeeze()\n",
    "\n",
    "    return model, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c9d5b0-a7d9-4ddb-a393-c68e705274fa",
   "metadata": {},
   "source": [
    "### Check Support Functions\n",
    "\n",
    "_Verify support function convergence._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7dea2f-3f94-4d0e-956d-d9a281666a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, r2 = fit_rastrigin(step_size=None, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056033df-c452-41c1-bdbf-01d4ca4a8a66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jenn.plot_convergence(histories=[model.history])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab369d1-1e1f-40d8-9b67-15293972a026",
   "metadata": {},
   "source": [
    "### Trade Study\n",
    "\n",
    "Plot how R-Squared varies as a function of finite difference step size and hyperparameter $\\gamma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2704761-cae3-45f4-be21-d1712ee7fe81",
   "metadata": {},
   "source": [
    "#### R-Squared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57425f49-02f2-43f5-9e5e-9e7b492ee2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "step_sizes = [\n",
    "    1e-6,\n",
    "    1e-4,\n",
    "    1e-2,\n",
    "    1e-1,\n",
    "    0.15,\n",
    "    0.20,\n",
    "    0.25,\n",
    "    0.30,\n",
    "    0.35,\n",
    "    0.40,\n",
    "    0.45,\n",
    "    0.50,\n",
    "    0.55,\n",
    "    0.60,\n",
    "    0.65,\n",
    "    0.75,\n",
    "    0.80,\n",
    "    0.85,\n",
    "    0.90,\n",
    "    0.95,\n",
    "    1.0,\n",
    "]\n",
    "gammas = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "r2 = np.zeros((len(gammas), len(step_sizes)))\n",
    "for j, step_size in enumerate(step_sizes):\n",
    "    for i, gamma in enumerate(gammas):\n",
    "        r2[i, j] = fit_rastrigin(step_size, gamma)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d2e67-291e-4d96-9fbb-cb2b4c91bf14",
   "metadata": {},
   "source": [
    "#### Finite Difference Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a5f1f-1ce6-4e7d-bd65-9588dd299168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "for j, step_size in enumerate(step_sizes):\n",
    "    (\n",
    "        (_, _, J_train),\n",
    "        (_, _, J_train_FD),\n",
    "        (_, _, _),\n",
    "    ) = generate_training_data(step_size)\n",
    "    error = (abs(J_train - J_train_FD) / J_train * 100).mean()\n",
    "    errors.append(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d30a9-00e7-44a2-8a8e-7f50d5b3a5d9",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48bcc81-fa3f-409b-b2a5-995ec275dd76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "stop = -10\n",
    "gamma = \"\\u03b3\"\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3.25, 3.0))\n",
    "plt.plot(errors[start:stop], r2[:, start:stop].T, \"o-\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Mean Finite Difference Error (%)\")\n",
    "plt.ylabel(\"R-Squared\")\n",
    "plt.legend([f\"{gamma} = {value}\" for value in gammas])\n",
    "# ax.set_xscale('log')\n",
    "fig.savefig(\"noisy_partials.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a83a38-a181-4b75-83db-84c7590bd6fc",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "On average, beyond 5% error in the partials, the benefit of gradient-enhancement vanishes but, below that threshold, JENN (where $\\gamma > 0$) still outperforms NN (where $\\gamma = 0$). This implies that partials need not be perfectly accurate, provided they point in the correct direction. However, beyond 5% error, the opposite is true: partials are too inaccurate to be useful and, in that case, simple NN are the better choice. The results further reveal that there is no benefit to settings the hyperparameter $\\gamma$ to values between zero and one; it does not mitigate the effect of noisy partials. Consequently,  $\\gamma$ should be treated as a binary hyperparameter: $\\gamma = 1$ for JENN and $\\gamma = 0$ for NN. \n",
    "\n",
    "> Note: There are only two reasons to set $\\gamma$ to values other than zero or one: polishing or incomplete partials. An example of the former is provided in the surrogate-based optimization notebook. The latter occurs when partials are only avialable for some inputs in the training data. In that case, the missing partials can be ignored from the learning problem by setting their corresponing $gamma$ to zero. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
