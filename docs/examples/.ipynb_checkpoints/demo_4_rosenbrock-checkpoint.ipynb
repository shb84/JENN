{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4: Surrogate-Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to train a neural net and use it to find the minimum of the Rosenbrock, located at $f(1, 1)=0$, defined as: \n",
    "\n",
    "$\n",
    "f(x_1, x_2) = (1 - x_1)^2 + 100 (x_2 - x_1^2)^ 2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import jenn\n",
    "from copy import deepcopy\n",
    "from typing import Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1  # Optionally, fix random seed for repeatability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Support Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a two-dimensional function, we can compare the contour plots of the predicted and true response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize(\n",
    "        f: Callable,  \n",
    "        dfdx: Callable,  \n",
    "        x0: Tuple[float, float] = (1.25, -1.75), \n",
    "        alpha: float = .2, \n",
    "        max_iter: int = 100_000,\n",
    "        ax: plt.Axes | None = None, \n",
    "        title: str = \"\", \n",
    "        levels: int = 100, \n",
    "        is_legend: bool = True, \n",
    "        beta_1: float = 0.9,\n",
    "        beta_2: float = 0.99,\n",
    "        tau: float = 0.5,\n",
    "        tol: float = 1e-06,\n",
    "        max_count: int = 1000,\n",
    "        epsilon_absolute: float = 1e-12,  \n",
    "        epsilon_relative: float = 1e-12, \n",
    "    ):\n",
    "    \"\"\"Check that optimizer yields correct answer for rosenbrock function.\"\"\"\n",
    "    \n",
    "    # Initial guess\n",
    "    x0 = np.array(x0).reshape((2, 1))\n",
    "\n",
    "    # Optimization\n",
    "    opt = jenn.core.optimization.ADAMOptimizer(\n",
    "        beta_1, beta_2, tau, tol, max_count\n",
    "    )\n",
    "    xf = opt.minimize(\n",
    "        x0, f, dfdx, \n",
    "        alpha=alpha, \n",
    "        max_iter=max_iter,\n",
    "        epsilon_absolute=epsilon_absolute,  \n",
    "        epsilon_relative=epsilon_relative, \n",
    "    )\n",
    "    yf = f(xf)\n",
    "\n",
    "    # For plotting contours\n",
    "    lb = -2.\n",
    "    ub = 2.\n",
    "    m = 100\n",
    "    x1 = np.linspace(lb, ub, m)\n",
    "    x2 = np.linspace(lb, ub, m)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    Y = np.zeros(X1.shape)\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, m):\n",
    "            X = np.array([\n",
    "                [X1[i, j]],\n",
    "                [X2[i, j]],\n",
    "            ])\n",
    "            Y[i, j] = f(X).squeeze()\n",
    "        \n",
    "    if ax is not None:\n",
    "        x1_his = np.array([x[0] for x in opt.vars_history]).squeeze()\n",
    "        x2_his = np.array([x[1] for x in opt.vars_history]).squeeze()\n",
    "        ax.plot(x1_his, x2_his)\n",
    "        ax.plot(x0[0], x0[1], '+', ms=15)\n",
    "        ax.plot(xf[0], xf[1], 'o')\n",
    "        ax.plot(np.array([1.]), np.array([1.]), 'x')\n",
    "        if is_legend: \n",
    "            ax.legend(['history', 'initial guess', 'predicted optimum', 'true optimum'])\n",
    "        ax.contour(X1, X2, Y, levels, cmap='RdGy')\n",
    "        ax.set_title(title)\n",
    "        bbox_props = dict(boxstyle='square,pad=0.3', facecolor=\"white\", alpha=0.8)\n",
    "        label = f\"  y={float(yf.squeeze()): .2f} \\nx1={float(xf[0].squeeze()): .2f} \\nx2={float(xf[1].squeeze()): .2f}\"\n",
    "        ax.annotate(\n",
    "            label, \n",
    "            (xf[0], xf[1]), xytext=(-100, 0), \n",
    "            textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle='->'),\n",
    "            bbox=bbox_props,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_contours(\n",
    "    nn: jenn.NeuralNet, \n",
    "    nn_enhanced: jenn.NeuralNet, \n",
    "    nn_polished: jenn.NeuralNet, \n",
    "    x0: Tuple[float, float] = (1.25, -1.75), \n",
    "    levels: int = 100,\n",
    "    **kwargs, \n",
    "): \n",
    "    \"\"\"Plot contours of True, JENN and NN side by side.\"\"\"\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    optimize(\n",
    "        f=jenn.synthetic_data.rosenbrock.compute, \n",
    "        dfdx=jenn.synthetic_data.rosenbrock.compute_partials, \n",
    "        x0=x0,\n",
    "        ax=ax[0, 0],\n",
    "        title=\"True\", \n",
    "        levels=levels,\n",
    "        **kwargs\n",
    "    )\n",
    "    optimize(\n",
    "        f=nn.predict, \n",
    "        dfdx=nn.predict_partials, \n",
    "        x0=x0,\n",
    "        ax=ax[1, 0], \n",
    "        is_legend=False, \n",
    "        title=\"NN\",\n",
    "        levels=levels,\n",
    "        **kwargs\n",
    "    )\n",
    "    optimize(\n",
    "        f=nn_enhanced.predict, \n",
    "        dfdx=nn_enhanced.predict_partials, \n",
    "        x0=x0,\n",
    "        ax=ax[1, 1],\n",
    "        is_legend=False,\n",
    "        title=\"JENN\",\n",
    "        levels=levels,\n",
    "        **kwargs\n",
    "    )\n",
    "    optimize(\n",
    "        f=nn_polished.predict, \n",
    "        dfdx=nn_polished.predict_partials, \n",
    "        x0=x0,\n",
    "        ax=ax[0, 1],\n",
    "        is_legend=False,\n",
    "        title=\"JENN (polished)\",\n",
    "        levels=levels,\n",
    "        **kwargs\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the domain over which we will collect synthetic training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lb = (-2.0, -2.0)\n",
    "ub = (2.0, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate some synthetic data that will be used to train our JENN model later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, y_train, dydx_train = jenn.utilities.sample(\n",
    "    f=jenn.synthetic_data.rosenbrock.compute, \n",
    "    f_prime=jenn.synthetic_data.rosenbrock.compute_partials, \n",
    "    m_random=100, \n",
    "    m_levels=17, \n",
    "    lb=lb, \n",
    "    ub=ub, \n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate some synthetic data that will be used to test the accuracy of the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test, y_test, dydx_test = jenn.utilities.sample(\n",
    "    f=jenn.synthetic_data.rosenbrock.compute, \n",
    "    f_prime=jenn.synthetic_data.rosenbrock.compute_partials, \n",
    "    m_random=1000, \n",
    "    m_levels=0, \n",
    "    lb=lb, \n",
    "    ub=ub, \n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training \n",
    "\n",
    "For comparison, let's train two models: a regular neural network and gradient-enhanced neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.8 s, sys: 208 ms, total: 35 s\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nn_standard = jenn.NeuralNet(\n",
    "    layer_sizes=[2] + [20] * 3 + [1],\n",
    ").fit(\n",
    "    x=x_train, \n",
    "    y=y_train, \n",
    "    alpha=1e-5, \n",
    "    lambd=1e-3, \n",
    "    is_normalize=True, \n",
    "    max_iter=75_000,\n",
    "    random_state=RANDOM_SEED, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian-Enhanced Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 588 ms, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nn_enhanced = jenn.NeuralNet(\n",
    "    layer_sizes=[2] + [20] * 3 + [1],\n",
    ").fit(\n",
    "    x=x_train, \n",
    "    y=y_train, \n",
    "    dydx=dydx_train,\n",
    "    alpha=1e-5, \n",
    "    lambd=1e-3,\n",
    "    is_normalize=True, \n",
    "    max_iter=75_000,\n",
    "    random_state=RANDOM_SEED, \n",
    "    shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian-Enhanced Neural Network (Polished)  \n",
    "\n",
    "Polishing starts from a previously trained model and continues training it on magnified portions of the design space where either function values or partials are very small. This is accomplished by using JENN's optional feature to set `gamma` as a distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_polished = deepcopy(nn_enhanced)  # to restart training without deleting previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nn_polished.fit(\n",
    "    x=x_train, \n",
    "    y=y_train, \n",
    "    dydx=dydx_train,\n",
    "    alpha=1e-5, \n",
    "    lambd=1e-3,\n",
    "    gamma=1 + 1e3 * jenn.utilities.rbf(dydx_train, epsilon=0.1),  # magnify near-zero partials\n",
    "    is_normalize=True, \n",
    "    max_iter=100_000,\n",
    "    random_state=RANDOM_SEED, \n",
    "    is_warmstart=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Goodness of Fit\n",
    "\n",
    "Upon convergence, let's check how well the model generalizes on test data. All else being equal, JENN generally outperforms NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jenn.plot_goodness_of_fit(\n",
    "    y_true=y_test, \n",
    "    y_pred=nn_standard.predict(x_test),\n",
    "    title=\"NN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jenn.plot_goodness_of_fit(\n",
    "    y_true=y_test, \n",
    "    y_pred=nn_enhanced.predict(x_test),\n",
    "    title=\"JENN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jenn.plot_goodness_of_fit(\n",
    "    y_true=y_test, \n",
    "    y_pred=nn_polished.predict(x_test),\n",
    "    title=\"JENN (polished)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Convergence Histories\n",
    "It is good practice to check the convergence history in order to decide if we should keep training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "his = jenn.plot_convergence(histories=[nn_standard.history, nn_enhanced.history, nn_polished.history], legend=[\"NN\", \"JENN\", \"JENN (polished)\"])\n",
    "his.savefig(\"sbo_convergence.pdf\")\n",
    "his"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate-Based Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plot_contours(\n",
    "    nn_standard, \n",
    "    nn_enhanced, \n",
    "    nn_polished,\n",
    "    x0=(1.25, -1.75), \n",
    "    levels=100, \n",
    "    alpha=0.05, \n",
    "    beta_1=0.9,\n",
    "    beta_2=0.99,\n",
    "    tau=0.5,\n",
    "    tol=1e-7,\n",
    "    max_count=2000,\n",
    ")\n",
    "fig.savefig(\"sbo_contours.pdf\")\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
